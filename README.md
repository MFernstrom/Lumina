![Lumina](media/lumina.png)  
[![Chat on Discord](https://img.shields.io/discord/754884471324672040?style=for-the-badge)](https://discord.gg/tPWjMwK)
[![Follow on Bluesky](https://img.shields.io/badge/Bluesky-tinyBigGAMES-blue?style=for-the-badge&logo=bluesky)](https://bsky.app/profile/tinybiggames.com)

# ğŸŒŸ Lumina: Advanced Local Generative AI for Delphi Developers ğŸ’»ğŸ¤–

Lumina offers a cutting-edge ğŸ› ï¸ for Delphi developers to seamlessly integrate advanced generative AI capabilities into their ğŸ“±. Built on the computational backbone of **llama.cpp** ğŸª, Lumina prioritizes data privacy ğŸ”’, performance âš¡, and a user-friendly API ğŸ“š, making it a powerful tool for local AI inference ğŸ¤–.

## ğŸ§ Why Choose Lumina?

- **Localized Processing** ğŸ : Operates entirely offline, ensuring sensitive data remains confidential ğŸ›¡ï¸ while offering complete computational control ğŸ§ .
- **Broad Model Compatibility** ğŸŒ: Supports **GGUF models** compliant with llama.cpp standards, granting access to diverse AI architectures ğŸ§©.
- **Intuitive Development Interface** ğŸ›ï¸: A concise, flexible API simplifies model management ğŸ—‚ï¸, inference execution ğŸ§®, and callback customization ğŸšï¸, minimizing implementation complexity.
- **Future-Ready Scalability** ğŸš€: This release emphasizes stability ğŸ—ï¸ and foundational features, with plans for multi-turn conversation ğŸ’¬ and retrieval-augmented generation (RAG) ğŸ” in future updates.

## ğŸ› ï¸ Key Functionalities

### ğŸ¤– Advanced AI Integration

Lumina expands your development toolkit ğŸ’ with capabilities such as:
- Dynamic chatbot creation ğŸ’¬.
- Automated text generation ğŸ“ and summarization ğŸ“°.
- Context-sensitive content generation âœï¸.
- Real-time inference for adaptive processes âš¡.

### ğŸ”’ Privacy-Driven AI Execution

- Operates independently of external networks ğŸ›¡ï¸, guaranteeing data security.
- Uses Vulkan ğŸ–¥ï¸ for optional GPU acceleration to enhance performance.

### âš™ï¸ Performance Optimization

- Configurable GPU utilization through the `AGPULayers` parameter ğŸ§©.
- Dynamic thread allocation based on hardware capabilities ğŸ–¥ï¸ via `AMaxThreads`.
- Comprehensive performance metrics ğŸ“Š, offering insights into throughput ğŸ“ˆ and efficiency.

### ğŸ”— Streamlined Integration

- Embedded dependencies eliminate the need for external libraries ğŸ“¦.
- Lightweight architecture (~2.5MB overhead) ensures broad deployment compatibility ğŸŒ.

## ğŸ“¥ Installation

1. **Download the Repository** ğŸ“¦
   - [Download here](https://github.com/tinyBigGAMES/Lumina/archive/refs/heads/main.zip) and extract the files to your preferred directory ğŸ“‚.

2. **Acquire a GGUF Model** ğŸ§ 
   - Obtain a model from [Hugging Face](https://huggingface.co), such as [Gemma 2.2B GGUF (Q8_0)](https://huggingface.co/bartowski/gemma-2-2b-it-abliterated-GGUF/resolve/main/gemma-2-2b-it-abliterated-Q8_0.gguf?download=true). Save it to a directory accessible to your application (e.g., `C:/LLM/GGUF`) ğŸ’¾.

3. **Ensure GPU Compatibility** ğŸ®
   - Verify Vulkan compatibility for enhanced performance âš¡. Adjust `AGPULayers` as needed to accommodate VRAM limitations ğŸ“‰.

4. **âœ¨ TLumina Class** 
   - ğŸ“œ Add `Lumina` to your `uses` section.  
   - ğŸ› ï¸ Create an instance of `TLumina`.  
   - ğŸš€ All functionality will then be at your disposal. That simple! ğŸ‰

5. **Explore Examples** ğŸ”
   - Check the `examples` directory for detailed usage demonstrations ğŸ“š.

## ğŸ› ï¸ Usage

### ğŸ”§ Basic Setup

Integrate Lumina into your Delphi project ğŸ–¥ï¸:

```delphi
var
  Lumina: TLumina;
begin
  Lumina := TLumina.Create;
  try
    if Lumina.LoadModel('C:\LLM\GGUF\gemma-2-2b-it-abliterated-Q8_0.gguf',
      '', 8192, -1, 8) then
    begin
      if Lumina.SimpleInference('What is the capital of Italy?') then
        WriteLn('Inference completed successfully.')
      else
        WriteLn('Error: ', Lumina.GetError);
    end;
  finally
    Lumina.Free;
  end;
end;
```

### ğŸšï¸ Customizing Callbacks

Define custom behavior using Luminaâ€™s callback functions ğŸ› ï¸:

```delphi
procedure NextTokenCallback(const AToken: string; const AUserData: Pointer);
begin
  Write(AToken);
end;

Lumina.SetNextTokenCallback(NextTokenCallback, nil);
```

## ğŸ“– API Reference

### ğŸ§© Core Methods

- **LoadModel** ğŸ“‚
  - Parameters:
    - `AModelFilename`: Path to the GGUF model file ğŸ“„.
    - `ATemplate`: Optional inference template ğŸ“.
    - `AMaxContext`: Maximum context size (default: 512) ğŸ§ .
    - `AGPULayers`: GPU layer configuration (-1 for maximum) ğŸ®.
    - `AMaxThreads`: Number of CPU threads allocated ğŸ–¥ï¸.
  - Returns a boolean indicating success âœ….

- **SimpleInference** ğŸ§ 
  - Accepts a single query for immediate processing ğŸ“.
  - Returns a boolean indicating success âœ….

- **SetNextTokenCallback** ğŸ’¬
  - Assigns a handler to process tokens during inference ğŸ§©.

- **UnloadModel** âŒ
  - Frees resources allocated during model loading ğŸ—‘ï¸.

- **GetPerformanceResult** ğŸ“Š
  - Provides metrics, including token generation rates ğŸ“ˆ.

## ğŸ› ï¸ Advanced Configurations

### ğŸ§  Custom Inference Templates

Lumina will use the template defined in the model's meta data by default, but you can also define custom templates to match your modelâ€™s requirements or change its behavor. These are some common model templates âœï¸:

```delphi
const
  CHATML_TEMPLATE = '<|im_start|>{role} {content}<|im_end|><|im_start|>assistant';
  GEMMA_TEMPLATE  = '<start_of_turn>{role} {content}<end_of_turn>';
  PHI_TEMPLATE    = '<|{role}|> {content}<|end|><|assistant|>';
```

- **{role}** - will be replaced with the role (user, assistant, etc.)
- **{content}** - will be replaced with the content sent to the model

### ğŸ® GPU Optimization

- `AGPULayers` values:
  - `-1`: Utilize all available layers (default) ğŸ–¥ï¸.
  - `0`: CPU-only processing ğŸ–¥ï¸.
  - Custom values for partial GPU utilization ğŸ›ï¸.

### ğŸ“Š Performance Metrics

Retrieve detailed operational metrics ğŸ“ˆ:

```delphi
var
  Perf: TLumina.PerformanceResult;
begin
  Perf := Lumina.GetPerformanceResult;
  WriteLn('Tokens/Sec: ', Perf.TokensPerSecond);
  WriteLn('Input Tokens: ', Perf.TotalInputTokens);
  WriteLn('Output Tokens: ', Perf.TotalOutputTokens);
end;
```

## ğŸ™ï¸ Media  

### ğŸŒŠ Deep Dive Podcast  
Discover in-depth discussions and insights about **Lumina** and its innovative features. ğŸš€âœ¨

https://github.com/user-attachments/assets/165e3dee-b29f-4478-b9ef-4fb6d2df2485


### ğŸ› ï¸ Support and Resources

- Report issues via the [Issue Tracker](https://github.com/tinyBigGAMES/Lumina/issues) ğŸ.
- Engage in discussions on the [Forum](https://github.com/tinyBigGAMES/Lumina/discussions) and [Discord](https://discord.gg/tPWjMwK) ğŸ’¬.
- Learn more at [Learn Delphi](https://learndelphi.org) ğŸ“š.

### ğŸ¤ Contributing  

Contributions to **âœ¨ Lumina** are highly encouraged! ğŸŒŸ  
- ğŸ› **Report Issues:** Submit issues if you encounter bugs or need help.  
- ğŸ’¡ **Suggest Features:** Share your ideas to make **Lumina** even better.  
- ğŸ”§ **Create Pull Requests:** Help expand the capabilities and robustness of the library.  

Your contributions make a difference! ğŸ™Œâœ¨

#### Contributors ğŸ‘¥ğŸ¤
<br/>

<a href="https://github.com/tinyBigGAMES/Lumina/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=tinyBigGAMES/Lumina&max=500&columns=20&anon=1" />
</a>

### ğŸ“œ Licensing

**Lumina** is distributed under the ğŸ†“ **BSD-3-Clause License**, allowing for redistribution and use in both source and binary forms, with or without modification, under specific conditions. See the [LICENSE](https://github.com/tinyBigGAMES/Lumina?tab=BSD-3-Clause-1-ov-file#BSD-3-Clause-1-ov-file) file for more details.

---

Advance your Delphi applications with Lumina ğŸŒŸ â€“ a sophisticated solution for integrating local generative AI ğŸ¤–.

<p align="center">
<img src="media/delphi.png" alt="Delphi">
</p>
<h5 align="center">

Made with :heart: in Delphi
</h5>

